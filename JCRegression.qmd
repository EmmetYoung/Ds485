---
title: "DS485Regression"
author: "Emmet Young"
format: pdf
editor: visual
---

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(janitor)
library(reshape2)
library(plotly)
library(magrittr)
library(tidyr)
library(RColorBrewer)
library(plyr)
library(stringr)
library(Hmisc)
library(qqplotr)
library(olsrr)
library(factoextra)
library(rsample)
library(caret)
library(rattle)
library(ggfortify)
library(glmnet)

```

```{r}
library(MASS)
library(class)
library(ggplot2)
```

# Cleaning and Importing

```{r}
soccer <- read_csv("KPIs.csv") %>% clean_names()
#importing data and creating clean names
```

```{r}
soccer <- soccer %>% mutate(formation = as.factor(formation))
#making formation a factor
```

```{r}
soccer <- soccer %>%
drop_na()
#dropping excess rows
```

```{r}
soccer$result <- ifelse(soccer$jc_goals > soccer$opponent_goals, "W",
               ifelse(soccer$jc_goals < soccer$opponent_goals, "L", "T"))
#Creating result
```

```{r}
soccer <- mutate(soccer, corner_kicks_percentage = ifelse(corner_kicks_percentage == "#DIV/0!", NA, corner_kicks_percentage))
```

```{r}
soccer <- soccer%>% 
  mutate(corner_kicks_percentage = as.numeric(corner_kicks_percentage))
```

# Goals Models

```{r}
#multiple linear regression for Juniata Goals looking at most correlated variables
jc_goals_model <- lm(jc_goals ~ wide_attacks_total + defensive_wide_attacks_total +  free_kicks_successful + box_touches + attacking_clear_cuts + opponent_clear_cuts +  won_off_backline + jc_shots + opponent_shots + opponent_goals + massey_ranking, data = soccer)

summary(jc_goals_model)
```

```{r}
#using best subset regression
soccer_best <- ols_step_best_subset(jc_goals_model)
soccer_best
```

```{r}
#5 looks optimal
plot(soccer_best)
```

```{r}

#using best subset
jc_goals_model_2 <- lm(jc_goals ~ defensive_wide_attacks_total + attacking_clear_cuts + opponent_clear_cuts + won_off_backline + opponent_goals, data = soccer  )
summary(jc_goals_model_2)

#interesting groups of variables to look at with a significant p-value
```

```{r}
autoplot(jc_goals_model)[1:2]
#not the best but might be good enough
```

```{r}
#not ideal but not horrible
autoplot(jc_goals_model_2)[1:2]
```

```{r}
#using no opponent statistics besides massey
jc_goals_model3 <- lm(jc_goals ~ wide_attacks_total +  free_kicks_successful + box_touches + attacking_clear_cuts +  won_off_backline + jc_shots  + massey_ranking + second_ball_won + backline_to_frontline , data = soccer)

summary(jc_goals_model3)
```

```{r}
soccer_best2 <- ols_step_best_subset(jc_goals_model3)
soccer_best2
```

```{r}
plot(soccer_best2)
#looks like 7
```

```{r}
jc_goals_model4 <- lm(jc_goals ~ wide_attacks_total+ attacking_clear_cuts + won_off_backline + jc_shots + massey_ranking + second_ball_won + backline_to_frontline, data = soccer)

summary(jc_goals_model4)

 
```

# Chances Models

```{r}
#chances model based on most correlated variables
jc_chances_model <- lm(attacking_clear_cuts ~ wide_attacks_won + defensive_wide_attacks_total + corner_kicks_total + backline_to_frontline + opponent_clear_cuts + free_kicks_successful + box_touches  +  won_off_backline + jc_shots + opponent_shots + opponent_goals + massey_ranking, data = soccer)

summary(jc_chances_model)
```

```{r}
#using best subset
chances_best <- ols_step_best_subset(jc_chances_model)
chances_best
```

```{r}
plot(chances_best)
#8 is best
```

```{r}
jc_chances_model2 <- lm(attacking_clear_cuts ~ defensive_wide_attacks_total+ backline_to_frontline + opponent_clear_cuts + free_kicks_successful + won_off_backline + jc_shots + opponent_goals + massey_ranking, data = soccer)

summary(jc_chances_model2)
```

```{r}
#without opponent characteristics besides massey ranking
jc_chances_model3 <- lm(attacking_clear_cuts ~  backline_to_frontline  + free_kicks_successful + won_off_backline + jc_shots + massey_ranking + wide_attacks_total, data = soccer)

summary(jc_chances_model3) 
```

```{r}
chances_best2 <- ols_step_best_subset(jc_chances_model3)
chances_best2
```

```{r}
plot(chances_best2)
```

```{r}
jc_chances_model4 <- lm(attacking_clear_cuts ~   free_kicks_successful + jc_shots + massey_ranking, data = soccer)

summary(jc_chances_model4)
```

# Predicting Win Loss

```{r}
#removes tie and makes binary becaus eonly 1 tie
soccer_logistic <- soccer %>% filter(result == "W"| result == "L")
soccer_logistic
```

```{r}

soccer_logistic$result <- ifelse(soccer_logistic$result == "W", 1, 0)

```

```{r}
#lasso regression for predicting formation

X <- soccer_logistic %>%
select(-formation, -opponent, - result, -corner_kicks_percentage) %>%
data.matrix() %>%
scale()

y <- soccer_logistic %>%
select(formation) %>%
data.matrix()
```

```{r}
n_lambdas <- 100
lambdas2 <- 10^seq(-4, 1, length.out = n_lambdas)
lambdas2
```

```{r}
lasso_mod <- glmnet(X, y, alpha = 1, lambda = lambdas2)

lasso_mod
```

```{r}
beta_lasso_mat <- lasso_mod$beta %>% as.matrix()

lasso_coef_df <- beta_lasso_mat %>%
t() %>%
as_tibble() %>%
mutate(lambda = lasso_mod$lambda) %>%
pivot_longer(-lambda, names_to = "predictor", values_to = "value")
```

```{r}
ggplot(lasso_coef_df, aes(x = lambda, y = value)) +
geom_line(aes(color = predictor)) +
labs(x = expression(lambda), y = expression(beta[k]), color =
"Predictor")

# does converge before 100
```

```{r}
set.seed(1)

cv_lasso_mod <- cv.glmnet(X, y, alpha = 1, lambda = lambdas2, nfolds = 10)

(best_lambda_2 <- cv_lasso_mod$lambda.min)
```

```{r}
#Not ideal, but most of the variables I looked at were very similar, so i guess it makes sense.
coef(lasso_mod, s = best_lambda_2)
```

```{r}
soccer_logistic_reg <- glm(result ~   opponent_goals +  opponent_shots  + defensive_wide_attacks_total, data = soccer_logistic, family = binomial())

summary(soccer_logistic_reg)
#few varaibles are even close to significant.
```

```{r}
ggplot(soccer_logistic, aes(x = massey_ranking, y = opponent_goals, color= result)) + 
  geom_point() +
    xlab("Massey Ranking") +
  ylab("Opponent Goals") + 
  theme_bw()

#Massey ranking should be a useful predictor
```

```{r}
head(soccer)
```

```{r}
soccer_logistic_reg <- glm(result ~  massey_ranking, data = soccer_logistic, 
                           family = binomial())

summary(soccer_logistic_reg)

#why is massey ranking not a useful predictor?
#it was just too useful

```

```{r}
preds <- predict(soccer_logistic_reg, soccer_logistic, type = "response")
```

```{r}
tibble(massey_ranking = soccer_logistic$massey_ranking,
  prob = preds,
  type = as.numeric(prob>= 0.50)) %>%
  ggplot()+ geom_point(aes(massey_ranking, type)) + geom_line(aes(massey_ranking, preds), color = "deeppink")+ xlab("Massey Ranking") + ylab("Probability of a Win") +
 theme_bw()
```

```{r}
soccer_logistic1 <- soccer_logistic[ -c(25) ]
head(soccer_logistic1)
```

```{r}
set.seed(1225549)
split <- initial_split(soccer_logistic1, prop = 0.8)

train_data <- training(split)
test_data <- testing(split)

```

```{r}
#| warning: false
#| message: false
rank_model1 <- train(train_data[, -c(1, 36, 37)],
                    train_data$result,
                    method = "rf",
                    trControl = trainControl(method="cv"))
```

```{r}
varImp(rank_model1$finalModel) 

```

```{r}
#| warning: false
#| message: false
rank_model <- train(soccer_logistic[, -c(1, 36,37)],
                    soccer_logistic$result,
                    method = "rpart",
                    trControl = trainControl(method="cv"))
```

```{r}

fancyRpartPlot(rank_model$finalModel) 
```

```{r}
soccer_qda <- qda(result ~ attacking_clear_cuts + opponent_clear_cuts, data = soccer_logistic)

soccer_qda
```

```{r}
# make prediction grid
x_range <- range(soccer_logistic$opponent_clear_cuts, na.rm = T)
y_range <- range(soccer_logistic$attacking_clear_cuts, na.rm = T)

# sequences
x_seq <- seq(x_range[1], x_range[2], length.out = 101)
y_seq <- seq(y_range[1], y_range[2], length.out = 101)

# make the grid of points
grid <- expand.grid(x_seq, y_seq)
names(grid) <- c("opponent_clear_cuts", "attacking_clear_cuts")
```

```{r}
grid$qda <- predict(soccer_qda, grid)$class
```

```{r}
soccer_logistic <- soccer_logistic%>% 
  mutate(result = as.factor(result))
```

```{r}
ggplot(soccer_logistic) + geom_point(aes(opponent_clear_cuts,attacking_clear_cuts, color = result)) + 
    geom_raster(data = grid, aes(opponent_clear_cuts, attacking_clear_cuts, fill = qda), alpha = 0.3) + 
    labs(x = "Opponent Clear Cuts", y = "Attacking Clear Cuts", fill = "Predicted Result", color = "Actual Result") +
    scale_fill_discrete(labels=c("Loss", "Win")) +
    scale_color_discrete(labels=c("Loss", "Win")) +
  theme_bw()
```

# Predicting Formation

```{r}
X1 <- soccer_logistic %>%
select(-result, -opponent, -formation, -corner_kicks_percentage) %>%
data.matrix() %>%
scale()

y1 <- soccer_logistic %>%
select(result) %>%
data.matrix()
```

```{r}
n_lambdas <- 100
lambdas3 <- 10^seq(-4, 1, length.out = n_lambdas)
lambdas3
```

```{r}
lasso_mod2 <- glmnet(X1, y1, alpha = 1, lambda = lambdas3)

lasso_mod2
```

```{r}
beta_lasso_mat2 <- lasso_mod2$beta %>% as.matrix()

lasso_coef_df2 <- beta_lasso_mat2 %>%
t() %>%
as_tibble() %>%
mutate(lambda = lasso_mod2$lambda) %>%
pivot_longer(-lambda, names_to = "predictor", values_to = "value")
```

```{r}
ggplot(lasso_coef_df, aes(x = lambda, y = value)) +
geom_line(aes(color = predictor)) +
labs(x = expression(lambda), y = expression(beta[k]), color =
"Predictor")
```

```{r}
set.seed(1)

cv_lasso_mod <- cv.glmnet(X, y, alpha = 1, lambda = lambdas2, nfolds = 10)

(best_lambda_2 <- cv_lasso_mod$lambda.min)
```

```{r}
coef(lasso_mod, s = best_lambda_2)
```

```{r}
soccer_logistic_reg2 <- glm(formation ~ attacking_clear_cuts + opponent_backline_to_frontline  + won_off_backline + opponent_clear_cuts, data = soccer_logistic, family = binomial())

summary(soccer_logistic_reg2)
```

```{r}
soccer_logistic_reg3 <- glm(formation ~ massey_ranking + attacking_clear_cuts + attacking_clear_cuts * massey_ranking, data = soccer_logistic, family = binomial())

summary(soccer_logistic_reg3)

#this was the best model I could find with linear regression, and I could not find significance, which makes sense because the medians were very similar when comparing the formations.
```
